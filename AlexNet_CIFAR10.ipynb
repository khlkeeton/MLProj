{"cells":[{"cell_type":"markdown","metadata":{"id":"UaX6ScG1ui6X"},"source":[]},{"cell_type":"markdown","metadata":{"id":"aAnCxtem9Bat"},"source":["# Resources\n","- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n","- https://blog.paperspace.com/alexnet-pytorch/\n","- https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py\n","- https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\n","- https://en.wikipedia.org/wiki/AlexNet\n","- http://d2l.ai/chapter_convolutional-modern/alexnet.html"]},{"cell_type":"markdown","metadata":{"id":"Np1-0oJV1N7A"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1714973580736,"user":{"displayName":"Syed Rizvi","userId":"03690040082444449942"},"user_tz":300},"id":"Ji1IDa77saLk","outputId":"9e7e3d34-3782-4c99-e9d1-7747285b2ef4"},"outputs":[],"source":["# import pandas as pd # dataframes\n","# import torchvision.transforms as transforms\n","\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torchmetrics\n","import torchvision\n","import torchvision.models as models\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","from torch.utils.tensorboard import SummaryWriter # http://localhost:6006/"]},{"cell_type":"markdown","metadata":{"id":"WMK_C_984PGq"},"source":["# Device Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1714973580736,"user":{"displayName":"Syed Rizvi","userId":"03690040082444449942"},"user_tz":300},"id":"TAMnRUzw4TJw","outputId":"bfea5c9d-92b6-41d2-b247-0356d82ac143"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using {device} processing\")\n","\n","#torch.set_default_dtype(torch.float16)"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameters & Loss Function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["learning_rates_test = [0.01, 0.005, 0.001,  0.0005, 0.0001] # learning rates to test\n","batch_size = 32\n","epoch_limit = 20\n","epoch_test_limit = 3\n","classes = 10\n","\n","loss_function = nn.CrossEntropyLoss() # Loss"]},{"cell_type":"markdown","metadata":{"id":"yINKpH7z1PN3"},"source":["# Load Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2208,"status":"ok","timestamp":1714973582933,"user":{"displayName":"Syed Rizvi","userId":"03690040082444449942"},"user_tz":300},"id":"Mk2kKREM4-24","outputId":"a803696d-7e4d-4888-c1ac-951749f37bc0"},"outputs":[],"source":["dir = './data'\n","download = True\n","transform = transforms.Compose( # define normalization transform\n","    [\n","      transforms.Resize((227,227)), # resize images to required minimum 227x227\n","      transforms.ToTensor(), # transform image to tensor and torch format\n","      transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5)) # apply normalize across all the channels for the image\n","    ]\n",")\n","\n","# Get Datasets\n","dataset_train = datasets.CIFAR10(root = dir, download=download, transform=transform, train = True)\n","dataset_test = datasets.CIFAR10(root = dir, download=download, transform=transform, train = False)\n","\n","# Split the training dataset into a training set (90% samples) and a validation set (10% samples).\n","size_train = int(0.9 * len(dataset_train))\n","size_valid = len(dataset_train) - size_train\n","\n","dataset_train, dataset_valid = torch.utils.data.random_split(dataset_train, [size_train, size_valid])\n","\n","from torch.utils.data import Subset\n","dataset_train = Subset(dataset_train, range(4000))\n","\n","# Data loaders split the data up into batches as determined by the batch size\n","loader_train = torch.utils.data.DataLoader(dataset = dataset_train, batch_size = batch_size, shuffle = True)\n","loader_valid = torch.utils.data.DataLoader(dataset = dataset_valid, batch_size = batch_size, shuffle = False)\n","\n","# Verify the sizes of the training and validation sets\n","print(f\"Training Size: {len(dataset_train)}\")\n","print(f\"Validation Size: {len(dataset_valid)}\")\n","print(f\"Testing Size: {len(dataset_test)}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Explore and Visualize Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def DisplayImage(image): # Normalize and display the image\n","    image = image / 2 + 0.5 # unnormalize\n","    npImage = image.numpy() # Convert the image tensor to a NumPy array\n","    plt.imshow(np.transpose(npImage, (1, 2, 0)))\n","    plt.show() # display\n","\n","dateIterator = iter(loader_train)\n","images, labels = next(dateIterator)\n","\n","DisplayImage(torchvision.utils.make_grid(images))\n"]},{"cell_type":"markdown","metadata":{"id":"12rnML7-1THu"},"source":["# Model Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HaOzGFEk19ik"},"outputs":[],"source":["model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"01Oyp3JV1Wrc"},"source":["# Define Training Function\n","- TensorBoard"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class MetricWrapper:\n","    def __init__(self, learning_rate, num_epochs, optimizer):\n","        self.learning_rate = learning_rate\n","        self.epochs = num_epochs\n","        self.optimizer = optimizer\n","\n","        self.losses = []\n","        self.accuracy = []\n","        self.f1 = []\n","\n","    def PrintMetrics(self):\n","        print(f'Trained with learning Rate: {self.learning_rate} for {self.epochs} epochs using the {self.optimizer} optimizer.')\n","        for epoch in range(self.epochs):\n","            print(f'Epoch {epoch+1}; Loss: {(self.losses[epoch]):.4f}; Accuracy: {(100 * self.accuracy[epoch]):.4f}%')\n","    \n","    def PrintFinal(self):\n","        print(f'Trained with learning Rate: {self.learning_rate} for {self.epochs} epochs using the {self.optimizer} optimizer.')\n","        print(f'Final Loss: {(self.losses[-1]):.4f}; ', end = '')\n","        print(f'Final Accuracy: {(100 * self.accuracy[-1]):.4f}%; ', end = '')\n","        print(f'Final F1: {(100 * self.f1[-1]):.4f}%')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":271,"status":"ok","timestamp":1714974132645,"user":{"displayName":"Syed Rizvi","userId":"03690040082444449942"},"user_tz":300},"id":"JzFTqtaM-Iy3"},"outputs":[],"source":["def TrainModel(optimizer_choice, batch_size, learning_rate, num_epochs, printStep = False):\n","  writer = SummaryWriter(log_dir=f'runs/{optimizer_choice}-{num_epochs}E-{learning_rate}LR')\n","  #print(f\"Training model using parameters:\")\n","  #print(f\"    - Optimizer: {optimizer_choice} \")\n","  #print(f\"    - Batch Size: {batch_size} \")\n","  #print(f\"    - Learning Rate: {learning_rate} \")\n","  #print(f\"    - Epochs: {num_epochs} \")\n","  #print()\n","\n","  # Model\n","  model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).to(device)\n","\n","  # Optimizer Function\n","  # this will help change the parameters of the model, influenced by the learning rate\n","  if optimizer_choice == 'Adam':\n","    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n","  elif optimizer_choice == 'SGD':\n","    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9)\n","  else:\n","    return\n","\n","  # Metrics\n","  metrics = MetricWrapper(learning_rate, num_epochs, optimizer_choice)\n","\n","  metric_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=classes).to(device)\n","  metric_f1 = torchmetrics.F1Score(task='multiclass', num_classes = classes).to(device)\n","\n","  # For Epoch\n","  for epoch in range(num_epochs):\n","    loss_epoch = 0\n","    for i, (inputs, targets) in enumerate(loader_train): # loop through each batch the dataloader has\n","      inputs = inputs.to(device) # Get the inputs and their target class\n","      targets = targets.to(device)\n","\n","      optimizer.zero_grad() # Zero the parameter gradients\n","\n","      outputs = model(inputs) # feed the model the inputs, and get predictions off the inputs\n","      loss = loss_function(outputs, targets) # compare the preditions to the actual target values of the inputs\n","      loss.backward() # compute the gradients\n","      optimizer.step() # actually update the model parameters based off the gradients computed previously\n","\n","      loss_epoch += loss.item()\n","\n","      if printStep and (i%(round((len(loader_train)/3), -1)) == 0):\n","        print(f'LR: {learning_rate}; Opt: {optimizer}; Epoch {epoch+1}/{num_epochs}; Step: {i+1}/{len(loader_train)}; Loss: {loss.item():.4f}')\n","        print()\n","    \n","    model.eval() # set the model to eval mode so it does not train off the whole training/testing sets\n","    with torch.no_grad(): # disabling gradient calculation, since we're not computing gradients\n","      metric_accuracy.reset()\n","      metric_f1.reset()\n","      \n","      for inputs, targets in loader_valid:\n","        inputs = inputs.to(device) # get test input\n","        targets = targets.to(device) # get the classes of the test input\n","        outputs = model(inputs) # predict the classification values of the test input\n","        predicted = torch.argmax(outputs.data, 1) # get the highest classification value\n","\n","        metric_accuracy.update(predicted, targets)\n","        metric_f1.update(predicted, targets)\n","\n","      loss_epoch = loss_epoch/len(loader_train)\n","      metrics.losses.append(loss_epoch)\n","      metrics.accuracy.append(metric_accuracy.compute().item())\n","      metrics.f1.append(metric_f1.compute().item())\n","\n","      writer.add_scalar(\"Loss/Epoch\", loss_epoch, epoch)\n","      writer.add_scalar(\"Accuracy/Epoch\", metric_accuracy.compute().item(), epoch)\n","\n","      #print(f'Epoch: {epoch+1}/{num_epochs}')\n","      #print(f'Loss: {loss_epoch:.4f}')\n","      #print(f'Accuracy: {(100 * metric_accuracy.compute()):.4f}%')\n","      #print(f'F1Score: {(100* metric_f1.compute()):.4f}%')\n","      #print()\n","\n","    model.train() # set model back in training mode\n","\n","  #print(f'Trained with learning Rate: {learning_rate} for {num_epochs} epochs using the {optimizer} optimizer.')\n","  #print(f'Final Loss: {(metrics.losses[-1]):.4f}; Final Accuracy: {(100 * metrics.accuracy[-1]):.4f}%; Final F1: {(100 * metrics.f1[-1]):.4f}%')\n","  writer.flush()\n","  return metrics"]},{"cell_type":"markdown","metadata":{},"source":["# Train Models"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":184169,"status":"error","timestamp":1714974593391,"user":{"displayName":"Syed Rizvi","userId":"03690040082444449942"},"user_tz":300},"id":"fOeDRd9VBZxq","outputId":"b79faa87-84f7-4e5b-d954-3f6f508a9cb5"},"outputs":[],"source":["param_training_start = time.time()\n","\n","test_metrics_adam = []\n","for learning_rate in learning_rates_test:\n","  test_metrics_adam.append(TrainModel('Adam', batch_size, learning_rate, epoch_test_limit))\n","  test_metrics_adam[-1].PrintFinal()\n","\n","test_metrics_sgd = []\n","for learning_rate in learning_rates_test:\n","  test_metrics_sgd.append(TrainModel('SGD', batch_size, learning_rate, epoch_test_limit))\n","  test_metrics_sgd[-1].PrintFinal()\n","\n","param_training_end = time.time()\n","\n","print(f\"Elapsed time: {(param_training_end - param_training_start):.2f} seconds ({((param_training_end - param_training_start)/60):.2f} minutes)\")\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for metrics in test_metrics_adam:\n","    metrics.PrintFinal()\n","    print()\n","\n","for metrics in test_metrics_sgd:\n","    metrics.PrintFinal()\n","    print()\n","\n","metrics_sorted = sorted(test_metrics_adam, key=lambda x: x.accuracy[-1], reverse=True)\n","learning_rates_adam = [wrapper.learning_rate for wrapper in metrics_sorted[:2]]\n","\n","metrics_sorted = sorted(test_metrics_sgd, key=lambda x: x.accuracy[-1], reverse=True)\n","learning_rates_sgd = [wrapper.learning_rate for wrapper in metrics_sorted[:2]]\n","\n","print('LR Adam')\n","for lr in learning_rates_adam:\n","    print(lr)\n","print()\n","print('LR SGD')\n","for lr in learning_rates_sgd:\n","    print(lr)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Evaluate Model Accuracy\n","- Visualizations"]},{"cell_type":"markdown","metadata":{},"source":["# Test Set Predictions"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":0}
