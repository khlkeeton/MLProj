{"cells":[{"cell_type":"markdown","metadata":{"id":"UaX6ScG1ui6X"},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# AlexNet CIFAR-100"]},{"cell_type":"markdown","metadata":{"id":"aAnCxtem9Bat"},"source":["# Resources\n","- https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n","- https://blog.paperspace.com/alexnet-pytorch/\n","- https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py\n","- https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html\n","- https://en.wikipedia.org/wiki/AlexNet\n","- http://d2l.ai/chapter_convolutional-modern/alexnet.html\n","- https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html"]},{"cell_type":"markdown","metadata":{"id":"Np1-0oJV1N7A"},"source":["# Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1714973580736,"user":{"displayName":"Syed Rizvi","userId":"03690040082444449942"},"user_tz":300},"id":"Ji1IDa77saLk","outputId":"9e7e3d34-3782-4c99-e9d1-7747285b2ef4"},"outputs":[],"source":["# import pandas as pd # dataframes\n","# import torchvision.transforms as transforms\n","\n","import time\n","import copy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torchmetrics\n","import torchvision\n","import torchvision.models as models\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","from torch.utils.tensorboard.writer import SummaryWriter # http://localhost:6006/ # tensorboard --logdir=runs # https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html"]},{"cell_type":"markdown","metadata":{"id":"WMK_C_984PGq"},"source":["# Device Configuration"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1714973580736,"user":{"displayName":"Syed Rizvi","userId":"03690040082444449942"},"user_tz":300},"id":"TAMnRUzw4TJw","outputId":"bfea5c9d-92b6-41d2-b247-0356d82ac143"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using {device} processing\")\n","\n","#torch.set_default_dtype(torch.float16)"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameters, Loss Function & Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["neural_net = \"AlexNet\"\n","dataset = \"CIFAR-100\"\n","run_directory = f\"runs/{neural_net}-{dataset}\"\n","\n","batch_size = 32\n","\n","epoch_tuning_limit = 3 # number of epochs to go for when tuning the hyperparameters\n","epoch_training_limit = 20 # number of epochs to go for when full training\n","\n","learning_rates_tuning = [0.01, 0.005, 0.001,  0.0005, 0.0001] # learning rates to tune\n","#learning_rates_adam = [] # to be determined by tuning\n","#learning_rates_sgd = [] # to be determined by tuning\n","\n","loss_function = nn.CrossEntropyLoss() # Loss function to use\n","\n","if dataset == 'CIFAR-10':\n","    classes = 10\n","elif dataset == 'CIFAR-100':\n","    classes = 100\n","\n","metric_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=classes, average='macro').to(device) # https://www.evidentlyai.com/classification-metrics/multi-class-metrics#:~:text=Macro%2Daveraging%20shows%20average%20performance,and%20accuracy%20are%20the%20same.\n","metric_f1 = torchmetrics.F1Score(task='multiclass', num_classes = classes, average='macro').to(device) # https://stackoverflow.com/questions/37358496/is-f1-micro-the-same-as-accuracy"]},{"cell_type":"markdown","metadata":{"id":"yINKpH7z1PN3"},"source":["# Load Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2208,"status":"ok","timestamp":1714973582933,"user":{"displayName":"Syed Rizvi","userId":"03690040082444449942"},"user_tz":300},"id":"Mk2kKREM4-24","outputId":"a803696d-7e4d-4888-c1ac-951749f37bc0"},"outputs":[],"source":["dir = './data'\n","download = True\n","transform = transforms.Compose( # define normalization transform\n","    [\n","      transforms.Resize((224,224)), # resize images to 224x224\n","      transforms.ToTensor(), # transform image to tensor and torch format\n","      transforms.Normalize(mean = (0.5, 0.5, 0.5), std = (0.5, 0.5, 0.5)) # apply normalize across all the channels for the image\n","      # mention why 0.5\n","    ]\n",")\n","\n","# Get Datasets\n","if dataset == 'CIFAR-10':\n","  dataset_train = datasets.CIFAR10(root = dir, download=download, transform=transform, train = True)\n","  dataset_test = datasets.CIFAR10(root = dir, download=download, transform=transform, train = False)\n","elif dataset == 'CIFAR-100':\n","  dataset_train = datasets.CIFAR100(root = dir, download=download, transform=transform, train = True)\n","  dataset_test = datasets.CIFAR100(root = dir, download=download, transform=transform, train = False)\n","class_names = dataset_train.classes\n","# Split the training dataset into a training set (90% samples) and a validation set (10% samples).\n","size_train = int(0.9 * len(dataset_train))\n","size_valid = len(dataset_train) - size_train\n","\n","dataset_train, dataset_valid = torch.utils.data.random_split(dataset_train, [size_train, size_valid])\n","\n","#from torch.utils.data import Subset # massively shortening training times to allow for testing of graphs\n","#dataset_train = Subset(dataset_train, range(1000))\n","#dataset_valid = Subset(dataset_valid, range(1000))\n","#dataset_test = Subset(dataset_test, range(1000))\n","\n","# Data loaders split the data up into batches as determined by the batch size\n","loader_train = torch.utils.data.DataLoader(dataset = dataset_train, batch_size = batch_size, shuffle = True, num_workers=4, pin_memory=True)\n","loader_valid = torch.utils.data.DataLoader(dataset = dataset_valid, batch_size = batch_size, shuffle = False, num_workers=4, pin_memory=True)\n","loader_test = torch.utils.data.DataLoader(dataset = dataset_test, batch_size = batch_size, shuffle = False, num_workers=4, pin_memory=True)\n","\n","# Verify the sizes of the training and validation sets\n","print(f\"Training Size: {len(dataset_train)}\")\n","print(f\"Validation Size: {len(dataset_valid)}\")\n","print(f\"Testing Size: {len(dataset_test)}\")\n","print(f'Num Classes: {len(class_names)}')"]},{"cell_type":"markdown","metadata":{},"source":["# Explore and Visualize Data\n","- We do basic data analysis on the dataset.\n","- As we are working with the CIFAR dataset, we can see that each image is an RBG image, with a single object as the focus.\n","- This object maps to one of the classes the CIFAR dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def DisplayImage(image): # Normalize and display the image\n","    image = image / 2 + 0.5 # unnormalize\n","    npImage = image.numpy() # Convert the image tensor to a NumPy array\n","    plt.imshow(np.transpose(npImage, (1, 2, 0)))\n","    plt.show() # display\n","\n","dateIterator = iter(loader_train)\n","images, labels = next(dateIterator)\n","\n","DisplayImage(torchvision.utils.make_grid(images))\n","print(labels) # to do\n","\n","# mean color distribtion, distribution of classes \n","# https://medium.com/@sehjadkhoja0/title-exploring-and-analyzing-image-data-with-python-79a7f72f4d2b"]},{"cell_type":"markdown","metadata":{"id":"12rnML7-1THu"},"source":["# Model Architecture\n","- We use the preset AlexNet model provided by PyTorch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HaOzGFEk19ik"},"outputs":[],"source":["model = models.AlexNet(num_classes=classes)\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"01Oyp3JV1Wrc"},"source":["# Define Training Function"]},{"cell_type":"markdown","metadata":{},"source":["## Model Wrapper\n","- We define a wrapper for our model that will help in storing, organzing, and persisting both the model and various metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ModelWrapper:\n","    def __init__(self, optimizer, batch_size, num_epochs, learning_rate):\n","        self.log_dir = f'{run_directory}/{optimizer}-{batch_size}BS-{num_epochs}E-{learning_rate}LR'\n","        self.writer = SummaryWriter(log_dir=self.log_dir)\n","\n","        self.optimizer = optimizer\n","        self.batch_size = batch_size\n","        self.epochs = num_epochs\n","        self.learning_rate = learning_rate\n","\n","        self.losses_train_step = [] # loss for a step during model training\n","        self.losses_train = [] # average loss on the training dataset for an epoch\n","        self.losses_valid = [] # loss on the validation dataset after an epoch\n","        self.loss_test = 0 # loss on the test dataset after a full training loop\n","        \n","        self.accuracy_train = [] # average accuracy on the training dataset for an epoch\n","        self.accuracy_valid = [] # accuracy on the validation dataset after an epoch\n","        self.accuracy_test = 0 # accuracy on the test dataset after a full training loop\n","        \n","        self.f1_train = [] # average f1 score on the training dataset for an epoch\n","        self.f1_valid = [] # f1 score on the validation dataset after an epoch\n","        self.f1_test = 0 # f1 score on the test dataset after a full training loop\n","\n","        self.start_time = time.time()\n","        self.end_time = time.time()\n","\n","        self.best_epoch = -1\n","        self.best_accuracy_valid = -1\n","        self.best_model_state = None\n","    \n","    def UpdateBestModel(self, model, at_epoch, validation_accuracy):\n","        self.best_model_state = copy.deepcopy(model.state_dict())\n","        self.best_epoch = at_epoch\n","        self.best_accuracy_valid = validation_accuracy\n","    \n","    def RunTime(self):\n","        return self.end_time - self.start_time\n","\n","    def PrintEpochs(self):\n","        print(f'Trained model with learning rate {self.learning_rate}, for {self.epochs} epochs, using the {self.optimizer} optimizer. {self.RunTime():.2f} seconds.')\n","        for epoch in range(self.epochs):\n","            print(f'Epoch {epoch+1}; Valid Loss {self.losses_valid[epoch]:.4f}; '\n","                  + f'Valid Loss: {self.losses_valid[epoch]:.4f}; '\n","                  + f'Valid Accuracy: {(100 * self.accuracy_valid[epoch]):.4f}%; ' \n","                  + f'Valid F1: {(100 * self.f1_valid[epoch]):.4f}%')\n","    \n","    def PrintFinal(self):\n","        print(f'Trained model with learning rate {self.learning_rate}, for {self.epochs} epochs, using the {self.optimizer} optimizer. {self.RunTime():.2f} seconds.')\n","        print(f'Final Train Loss:       {(self.losses_train[-1]):.4f};     ', end = '')\n","        print(f'Final Valid Loss:       {(self.losses_valid[-1]):.4f}')\n","        print(f'Final Train Accuracy:   {(100 * self.accuracy_train[-1]):.4f}%;      ', end = '')\n","        print(f'Final Valid Accuracy:   {(100 * self.accuracy_valid[-1]):.4f}%')\n","        print(f'Final Train F1:         {(100 * self.f1_train[-1]):.4f}%;        ', end = '')\n","        print(f'Final Valid F1:         {(100 * self.f1_valid[-1]):.4f}%')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":271,"status":"ok","timestamp":1714974132645,"user":{"displayName":"Syed Rizvi","userId":"03690040082444449942"},"user_tz":300},"id":"JzFTqtaM-Iy3"},"outputs":[],"source":["def TrainModel(optimizer_choice, batch_size, learning_rate, num_epochs, printStep = False, pretrained = False, save_best_model = False):\n","  torch.cuda.empty_cache()\n","  \n","  # Model\n","  if pretrained:\n","    model = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).to(device)\n","    model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, classes).to(device) # pretrained AlexNet is meant to output 1000\n","  else:\n","    model = models.AlexNet(num_classes=classes).to(device)\n","\n","  # Wrapper\n","  wrapper = ModelWrapper(optimizer=optimizer_choice, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate)\n","\n","  # Optimizer Function\n","  # this will help change the parameters of the model, influenced by the learning rate\n","  if optimizer_choice == 'Adam':\n","    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n","  elif optimizer_choice == 'SGD':\n","    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, momentum=0.9)\n","  else:\n","    return\n","\n","  # For Epoch\n","  for epoch in range(num_epochs):\n","\n","    loss_total = 0.0 # represents summation of the loss at each step for the validation dataset\n","    metric_accuracy.reset()\n","    metric_f1.reset()\n","    for i, (inputs, targets) in enumerate(loader_train): # loop through each batch the dataloader has\n","      inputs = inputs.to(device) # the input images from the training dataset \n","      targets = targets.to(device) # the class labels from the training dataset \n","\n","      optimizer.zero_grad() # Zero the parameter gradients\n","\n","      outputs = model(inputs) # feed the model the inputs, and get predictions off the inputs\n","      loss = loss_function(outputs, targets) # compare the preditions to the actual target values of the inputs\n","      loss.backward() # compute the gradients\n","      optimizer.step() # actually update the model parameters based off the gradients computed previously\n","\n","      predicted = torch.argmax(outputs.data, 1) # get the highest classification value\n","      metric_accuracy.update(predicted, targets) # update the accuracy with the current step info\n","      metric_f1.update(predicted, targets) # update the f1 with the current step info\n","      \n","      loss_total += loss.item() # add the step loss to the total loss\n","      wrapper.losses_train_step.append(loss.item()) # update loss training step\n","\n","      if printStep and (i%(round((len(loader_train)/3), -1)) == 0):\n","        print(f'LR: {learning_rate}; Opt: {optimizer}; Epoch {epoch+1}/{num_epochs}; Step: {i+1}/{len(loader_train)}; Loss: {loss.item():.4f}')\n","        print()\n","\n","    loss_average = loss_total / len(loader_valid) # calculate the average loss for the validation dataset this epoch\n","    accuracy_val = metric_accuracy.compute().item() # calculate the accuracy for the validation dataset this epoch\n","    f1_val = metric_f1.compute().item() # calculate the f1 score for the validation dataset this epoch\n","\n","    if save_best_model:\n","      a = ''\n","\n","    wrapper.losses_train.append(loss_average) # update valid loss epoch\n","    wrapper.accuracy_train.append(accuracy_val) # update valid accuracy epoch\n","    wrapper.f1_train.append(f1_val) # update valid f1 epoch\n","\n","    wrapper.writer.add_scalar(f\"Loss/Train/{num_epochs}E\", loss_average, epoch) # update tensorboard\n","    wrapper.writer.add_scalar(f\"Accuracy/Train/{num_epochs}E\", accuracy_val, epoch) # update tensorboard\n","    wrapper.writer.add_scalar(f\"F1/Train/{num_epochs}E\", f1_val, epoch) # update tensorboard\n","    \n","    model.eval() # set the model to eval mode so it does not train off the whole training/testing sets\n","    with torch.no_grad(): # disabling gradient calculation, since we're not computing gradients # get the validation dataset metrics\n","      loss_total = 0.0 # represents summation of the loss at each step for the validation dataset\n","      metric_accuracy.reset()\n","      metric_f1.reset()\n","      for inputs, targets in loader_valid: \n","        inputs = inputs.to(device) # input images from the validation dataset\n","        targets = targets.to(device) # get the classes of the test input\n","        outputs = model(inputs) # predict the classification values of the test input\n","        predicted = torch.argmax(outputs.data, 1) # get the highest classification value\n","\n","        loss = loss_function(outputs, targets) # compute the loss of the step\n","        loss_total += loss.item() # add the step loss to the total loss\n","\n","        metric_accuracy.update(predicted, targets) # update the accuracy with the current step info\n","        metric_f1.update(predicted, targets) # update the f1 with the current step info\n","\n","      loss_average = loss_total / len(loader_valid) # calculate the average loss for the validation dataset this epoch\n","      accuracy_val = metric_accuracy.compute().item() # calculate the accuracy for the validation dataset this epoch\n","      f1_val = metric_f1.compute().item() # calculate the f1 score for the validation dataset this epoch\n","\n","      if save_best_model and (accuracy_val > wrapper.best_accuracy_valid): # best model\n","        wrapper.UpdateBestModel(model=model, at_epoch=epoch, validation_accuracy=accuracy_val)\n","\n","      wrapper.losses_valid.append(loss_average) # update valid loss epoch\n","      wrapper.accuracy_valid.append(accuracy_val) # update valid accuracy epoch\n","      wrapper.f1_valid.append(f1_val) # update valid f1 epoch\n","      \n","      wrapper.writer.add_scalar(f\"Loss/Valid/{num_epochs}E\", loss_average, epoch) # update tensorboard\n","      wrapper.writer.add_scalar(f\"Accuracy/Valid/{num_epochs}E\", accuracy_val, epoch) # update tensorboard\n","      wrapper.writer.add_scalar(f\"F1/Valid/{num_epochs}E\", f1_val, epoch) # update tensorboard\n","\n","    model.train() # set model back in training mode\n","\n","  wrapper.writer.flush()\n","  wrapper.end_time = time.time()\n","  return wrapper"]},{"cell_type":"markdown","metadata":{},"source":["# Hyperparameter Tuning\n","- We train multiple models from the tuning learning rate values both optimizers over a limited number of epochs.\n","- We will select the two most promising learning rates for each optimizer and run those learning rates to the specified epoch training limit"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":184169,"status":"error","timestamp":1714974593391,"user":{"displayName":"Syed Rizvi","userId":"03690040082444449942"},"user_tz":300},"id":"fOeDRd9VBZxq","outputId":"b79faa87-84f7-4e5b-d954-3f6f508a9cb5"},"outputs":[],"source":["print(f'Training {len(learning_rates_tuning)} learning rates for both Adam and SGD loss functions over {epoch_tuning_limit} epochs.')\n","time_tuning_start = time.time()\n","\n","models_tuning_adam = []\n","for learning_rate in learning_rates_tuning:\n","  models_tuning_adam.append(TrainModel(optimizer_choice='Adam', batch_size=batch_size, num_epochs=epoch_tuning_limit, learning_rate=learning_rate))\n","\n","models_tuning_sgd = []\n","for learning_rate in learning_rates_tuning:\n","  models_tuning_sgd.append(TrainModel(optimizer_choice='SGD', batch_size=batch_size, num_epochs=epoch_tuning_limit, learning_rate=learning_rate))\n","\n","time_tuning_end = time.time()\n","time_tuning = time_tuning_end - time_tuning_start\n","\n","print(f\"Training finished, elapsed time: {time_tuning:.2f} seconds ({(time_tuning/60):.2f} minutes)\")"]},{"cell_type":"markdown","metadata":{},"source":["## Selecting Learning Rates for Each Optimizer\n","- See which learning rates provide the highest validation accuracy after the specified number of tuning epochs.\n","- Select two learning rates which result in the highest validation accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('Adam Metrics')\n","for metrics in models_tuning_adam:\n","    metrics.PrintFinal()\n","    print()\n","\n","print('SGD Metrics')\n","for metrics in models_tuning_sgd:\n","    metrics.PrintFinal()\n","    print()\n","\n","learning_rate_adam = max(models_tuning_adam, key=lambda x: x.accuracy_valid[-1]).learning_rate\n","learning_rate_sgd = max(models_tuning_sgd, key=lambda x: x.accuracy_valid[-1]).learning_rate\n","\n","print(f'Adam LR: {learning_rate_adam}')\n","print(f'SGD LR: {learning_rate_sgd}')"]},{"cell_type":"markdown","metadata":{},"source":["# Train Models\n","- We train the selected learning rates for both optimizers to the number of epochs defined by the epoch_training_limit"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f'Training two models over {epoch_training_limit} epochs.')\n","time_training_start = time.time()\n","\n","model_adam = TrainModel(optimizer_choice='Adam', batch_size=batch_size, num_epochs=epoch_training_limit, learning_rate=learning_rate_adam, save_best_model=True)\n","model_sgd = TrainModel(optimizer_choice='SGD', batch_size=batch_size, num_epochs=epoch_training_limit, learning_rate=learning_rate_sgd, save_best_model=True)\n","\n","time_training_end = time.time()\n","time_training = time_training_end - time_training_start\n","\n","print(f\"Training finished, elapsed time: {time_training:.2f} seconds ({(time_training/60):.2f} minutes)\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('Adam Metrics')\n","model_adam.PrintEpochs()\n","print()\n","\n","print('SGD Metrics')\n","model_sgd.PrintEpochs()\n","print()"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluate Model Accuracy\n","- We evaluate the accuracy of the model on both the training and validation datasets.\n","- We compare the accuracy of the selected learning rates within each optimizer.\n","- We compare the accuracy of the two optimizers against each other"]},{"cell_type":"markdown","metadata":{},"source":["## Define Plot Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def PlotAccuracy(wrapper):\n","    plt.close()\n","    range_epochs = range(1, epoch_training_limit+1)\n","    \n","    plt.plot(range_epochs, [value * 100 for value in wrapper.accuracy_train], label=f'Training') # training accuracy\n","    plt.plot(range_epochs, [value * 100 for value in wrapper.accuracy_valid], label=f'Validation') # validation accuracy\n","\n","    plt.suptitle(f'{neural_net}-{dataset}') # Super Title\n","    plt.title(f'{wrapper.optimizer} optimizer using learning rate: {wrapper.learning_rate}') # Title\n","    plt.legend(bbox_to_anchor=(0.5, -0.2), loc='upper center', ncol=2)\n","    plt.grid(True)\n","\n","    plt.xlabel('Epochs')\n","    plt.xlim(1, epoch_training_limit)\n","    plt.xticks(range_epochs)\n","\n","    plt.ylabel('Accuracy (%)')\n","    plt.ylim(0, 100)\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def PlotCompareAccuracy(wrappers):\n","    plt.close()\n","    range_epochs = range(1, epoch_training_limit+1)\n","    \n","    for wrapper in wrappers:\n","        plt.plot(range_epochs, [value * 100 for value in wrapper.accuracy_train], label=f'Optimal {wrapper.optimizer} Training') # training accuracy\n","        plt.plot(range_epochs, [value * 100 for value in wrapper.accuracy_valid], label=f'Optimal {wrapper.optimizer} Validation') # validation accuracy\n","    # test set loss\n","\n","    plt.suptitle(f'{neural_net}-{dataset}') # Super Title\n","    plt.title(f'Comparing Adam vs SGD optimizers') # Title\n","    plt.legend(bbox_to_anchor=(0.5, -0.2), loc='upper center', ncol=2)\n","    plt.grid(True)\n","\n","    plt.xlabel('Epochs')\n","    plt.xlim(1, epoch_training_limit)\n","    plt.xticks(range_epochs)\n","\n","    plt.ylabel('Accuracy (%)')\n","    plt.ylim(0, 100)\n","\n","    plt.show()\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Adam Optimizer Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PlotAccuracy(model_adam)"]},{"cell_type":"markdown","metadata":{},"source":["## SGD Optimizer Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PlotAccuracy(model_sgd)"]},{"cell_type":"markdown","metadata":{},"source":["## Adam vs SGD Accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["PlotCompareAccuracy([model_adam, model_sgd])"]},{"cell_type":"markdown","metadata":{},"source":["# Predictions on Test Set\n","- We get the loss, the accuracy, and the F1 score of the model against the test set.\n","- These are not definitive metrics, as the test set is only evaluated after fully training the model as defined by epoch_test_limit.\n","- This means that the model could have overfitted by the number of epochs specified."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def PredictTestSet(wrapper):\n","    model = models.AlexNet(num_classes=classes)\n","    model.load_state_dict(wrapper.best_model_state)\n","    model = model.to(device)\n","\n","    print(f'Copied best model at {wrapper.best_epoch} epoch for {wrapper.optimizer} optimizer, using LR: {wrapper.learning_rate}')\n","    print(f'Best Accuracy: {(100 * wrapper.best_accuracy_valid):.2f}%')\n","    with torch.no_grad():\n","        loss_total = 0.0\n","        metric_accuracy.reset()\n","        metric_f1.reset()\n","\n","        for inputs, targets in loader_test:\n","            inputs = inputs.to(device)\n","            targets = targets.to(device)\n","            outputs = model(inputs)\n","            predicted = torch.argmax(outputs.data, 1)\n","\n","            loss = loss_function(outputs, targets)\n","            loss_total += loss.item()\n","\n","            metric_accuracy.update(predicted, targets)\n","            metric_f1.update(predicted, targets)\n","\n","        wrapper.loss_test = loss_total / len(loader_test)\n","        wrapper.accuracy_test = metric_accuracy.compute().item()\n","        wrapper.f1_test = metric_f1.compute().item()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for wrapper in [model_adam, model_sgd]:\n","    PredictTestSet(wrapper)\n","\n","for wrapper in [model_adam, model_sgd]:\n","    print(f'Opt: {wrapper.optimizer}; LR: {wrapper.learning_rate}')\n","    print(wrapper.loss_test)\n","    print(wrapper.accuracy_test)\n","    print(wrapper.f1_test)\n","    print()"]},{"cell_type":"markdown","metadata":{},"source":["# Export Graphs for Report\n","- Loss/Epoch\n","- Accuracy/Epoch\n","- F1/Epoch\n","- Loss/Step\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["xticks_val = [1, 5, 10, 15, 20]\n","range_epochs = range(1, epoch_training_limit+1)\n","\n","for wrapper in (model_adam, model_sgd): # Training and Valid Loss\n","    plt.close()\n","    \n","    plt.plot(range_epochs, wrapper.losses_train, label='Training Loss', color='blue')\n","    plt.plot(range_epochs, wrapper.losses_valid, label='Validation Loss', color='red')\n","\n","    plt.xlabel('Epochs')\n","    plt.xlim(1, epoch_training_limit)\n","    plt.xticks(xticks_val)\n","\n","    plt.ylabel('Loss')\n","    \n","    plt.suptitle(f'{neural_net}-{dataset}') # Super Title\n","    plt.title(f'{wrapper.optimizer} Optimizer with Learning Rate {wrapper.learning_rate}') # Title\n","    plt.legend()\n","\n","    plt.savefig(f'out/{neural_net}-{dataset}/{wrapper.optimizer}/{wrapper.learning_rate}-loss-epoch.png')\n","\n","for wrapper in (model_adam, model_sgd): # Validation Accuracy\n","    plt.close()\n","\n","    plt.plot(range_epochs, [value * 100 for value in wrapper.accuracy_train], label='Training Accuracy', color='blue')\n","    plt.plot(range_epochs, [value * 100 for value in wrapper.accuracy_valid], label='Validation Accuracy', color='red')\n","\n","    plt.title(f'{wrapper.optimizer} Optimizer with Learning Rate {wrapper.learning_rate}')\n","    plt.suptitle(f'{neural_net}-{dataset}')\n","\n","    plt.xlabel('Epochs')\n","    plt.xlim(1, epoch_training_limit)\n","    plt.xticks(xticks_val)\n","\n","    plt.ylabel('Accuracy (%)')\n","    plt.ylim(0, 100)\n","\n","    plt.legend()\n","\n","    plt.savefig(f'out/{neural_net}-{dataset}/{wrapper.optimizer}/{wrapper.learning_rate}-acc-epoch.png')\n","\n","# F1 Score/Epoch\n","for wrapper in (model_adam, model_sgd):\n","    plt.close()\n","\n","    plt.plot(range_epochs, [value * 100 for value in wrapper.f1_train], label='Training F1 Score', color='blue')\n","    plt.plot(range_epochs, [value * 100 for value in wrapper.f1_valid], label='Validation F1 Score', color='red')\n","\n","    plt.title(f'{wrapper.optimizer} Optimizer with learning rate {wrapper.learning_rate}')\n","    plt.suptitle(f'{neural_net}-{dataset}')\n","\n","    plt.xlabel('Epochs')\n","    plt.xlim(1, epoch_training_limit)\n","    plt.xticks(xticks_val)\n","\n","    plt.ylabel('F1 Score (%)')\n","    plt.ylim(0, 100)\n","\n","    plt.legend()\n","\n","    plt.savefig(f'out/{neural_net}-{dataset}/{wrapper.optimizer}/{wrapper.learning_rate}-f1-epoch.png')\n","\n","\n","for wrapper in (model_adam, model_sgd): # Training Loss/Step\n","    plt.close()\n","    \n","    # Data\n","    range_steps = range(1, len(wrapper.losses_train_step)+1)\n","    plt.plot(range_steps, wrapper.losses_train_step, label='Training Loss/Steps', color='blue')\n","\n","    plt.xlabel('Steps')\n","    plt.xlim(1, len(wrapper.losses_train_step))\n","\n","    plt.ylabel('Loss')\n","    \n","    plt.suptitle(f'{neural_net}-{dataset}') # Super Title\n","    plt.title(f'{wrapper.optimizer} Optimizer with learning rate {wrapper.learning_rate}') # Title\n","    plt.legend()\n","\n","    #plt.show()\n","    plt.savefig(f'out/{neural_net}-{dataset}/{wrapper.optimizer}/{wrapper.learning_rate}-loss-step.png')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":0}
